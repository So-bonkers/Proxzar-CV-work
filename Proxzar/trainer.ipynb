{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54966228-40f2-415f-ac3d-247baea2ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from glob import glob\n",
    "# from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from patchify import patchify\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping\n",
    "from keras import layers\n",
    "from keras import models, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import UnidentifiedImageError  # Import UnidentifiedImageError explicitly\n",
    "from vit import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "415f1fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957b8dc9-70c7-404c-8a2f-4c800dcd2f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters dictionary\n",
    "hp = {}\n",
    "hp[\"image_size\"] = 256\n",
    "hp[\"num_channels\"] = 3 \n",
    "hp[\"patch_size\"] = 32\n",
    "hp[\"num_patches\"] = (hp[\"image_size\"]**2) // (hp[\"patch_size\"]**2)\n",
    "hp[\"flat_patches_shape\"] = (hp[\"num_patches\"], hp[\"patch_size\"]*hp[\"patch_size\"]*hp[\"num_channels\"])\n",
    "\n",
    "hp[\"batch_size\"] = 64\n",
    "hp[\"learning_rate\"] = 1e-4\n",
    "hp[\"num_epochs\"] = 500\n",
    "hp[\"num_classes\"] = 7\n",
    "hp[\"class_names\"] = [\"gutters\",\"roofing\",\"insulation\",\"waterproofing\",\"tools-equipment\",\"siding\",\"building-materials\"]\n",
    "\n",
    "hp[\"num_layers\"] = 12\n",
    "hp[\"hidden_dim\"] = 768\n",
    "hp[\"mlp_dim\"] = 3072\n",
    "hp[\"num_heads\"] = 12\n",
    "hp[\"dropout_rate\"] = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9abe371-d0f7-41be-b272-c2f27be18f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hp keys: dict_keys(['image_size', 'num_channels', 'patch_size', 'num_patches', 'flat_patches_shape', 'batch_size', 'learning_rate', 'num_epochs', 'num_classes', 'class_names', 'num_layers', 'hidden_dim', 'mlp_dim', 'num_heads', 'dropout_rate'])\n",
      "Batch size: 64\n",
      "Flat patches shape:  (64, 3072)\n",
      "Number of patches:  64\n"
     ]
    }
   ],
   "source": [
    "print(\"hp keys:\", hp.keys())  # Check all keys in hp\n",
    "print(\"Batch size:\", hp.get(\"batch_size\"))  # Check the value associated with \"batch_size\"\n",
    "print(\"Flat patches shape: \", hp.get(\"flat_patches_shape\"))\n",
    "print(\"Number of patches: \", hp.get(\"num_patches\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d508a9e8-688e-4eae-ba09-2302660772f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3b2cec6-428a-40ae-adae-3532a9771271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, split=0.1):\n",
    "    images = shuffle(glob(os.path.join(path, \"*\", \"*.jpg\")))\n",
    "\n",
    "    split_size = int(len(images) * split)\n",
    "    train_x, valid_x = train_test_split(images, test_size=split, random_state=42)\n",
    "    train_x, test_x = train_test_split(train_x, test_size=split, random_state=42)\n",
    "    \n",
    "    # Additional processing or modifications as needed\n",
    "\n",
    "    return train_x, valid_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59e255bd-dde0-4ba9-a5af-65a6f2c8439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_label(path):\n",
    "    # Reading Images\n",
    "    path = path.decode()\n",
    "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.resize(image, (hp[\"image_size\"], hp[\"image_size\"]))\n",
    "    image = image/255.0\n",
    "    \n",
    "    # print(\"Image shape is: \", image.shape)\n",
    "    # cv2.imwrite(f\"files/original_image.jpg\", image)\n",
    "\n",
    "    # Preprocessing to patches\n",
    "    patch_shape = (hp[\"patch_size\"], hp[\"patch_size\"], hp[\"num_channels\"])\n",
    "    patches = patchify(image, patch_shape, hp[\"patch_size\"])\n",
    "\n",
    "    # patches = np.reshape(patches, (64,32,32,3))\n",
    "    # print(\"Patches shape is: \", patches.shape)\n",
    "    # for i in range(64):\n",
    "    #     cv2.imwrite(f\"files/{i}.png\", patches[i])\n",
    "\n",
    "    patches = np.reshape(patches, hp[\"flat_patches_shape\"])\n",
    "    patches = patches.astype(np.float32)\n",
    "\n",
    "    # Label\n",
    "    # print(path)\n",
    "    class_name = path.split(\"/\")[-2]\n",
    "    # print(\"Class name: \",class_name)\n",
    "    class_index = hp[\"class_names\"].index(class_name)\n",
    "    class_index = np.array(class_index, dtype=np.int32)\n",
    "    # print(\"Class Index: \", class_index)\n",
    "\n",
    "    return patches, class_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99d502b8-ef44-40c1-a05b-80ab4538ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    patches, labels = tf.numpy_function(process_image_label, [path], [tf.float32, tf.int32])\n",
    "    labels = tf.one_hot(labels, hp[\"num_classes\"])\n",
    "\n",
    "    patches.set_shape(hp[\"flat_patches_shape\"])\n",
    "    labels.set_shape(hp[\"num_classes\"])\n",
    "\n",
    "    return patches, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28845d55-1b38-4214-a9b5-4355e1cd6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_dataset(images, batch=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images))\n",
    "    ds = ds.map(parse).batch(batch).prefetch(8)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a256dc89-8c94-4d49-ab0e-24ba4bf881b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m csv_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mfiles\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlog.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Dataset\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m train_x, valid_x, test_x \u001b[39m=\u001b[39m load_data(dataset_path)\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_x)\u001b[39m}\u001b[39;00m\u001b[39m - Valid: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(valid_x)\u001b[39m}\u001b[39;00m\u001b[39m - Test: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(test_x)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[39m# process_image_label(train_x[0])\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(path, split)\u001b[0m\n\u001b[0;32m      2\u001b[0m images \u001b[39m=\u001b[39m shuffle(glob(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m*.jpg\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[0;32m      4\u001b[0m split_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(images) \u001b[39m*\u001b[39m split)\n\u001b[1;32m----> 5\u001b[0m train_x, valid_x \u001b[39m=\u001b[39m train_test_split(images, test_size\u001b[39m=\u001b[39;49msplit, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m train_x, test_x \u001b[39m=\u001b[39m train_test_split(train_x, test_size\u001b[39m=\u001b[39msplit, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Additional processing or modifications as needed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\FactorizedQG\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    215\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\FactorizedQG\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2649\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2646\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[0;32m   2648\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m-> 2649\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2650\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[0;32m   2651\u001b[0m )\n\u001b[0;32m   2653\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m   2654\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\FactorizedQG\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2305\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2302\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[0;32m   2304\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2305\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2306\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2307\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2308\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2309\u001b[0m     )\n\u001b[0;32m   2311\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Seeding\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    # Directory for storing files\n",
    "    create_dir(\"files\")\n",
    "\n",
    "    # Paths\n",
    "    dataset_path = \"/home/sksystem/Downloads/Proxzar-CV-work-main/Final_folder_for_classification\"\n",
    "    model_path = os.path.join(\"files\", \"model.h5\")\n",
    "    csv_path = os.path.join(\"files\", \"log.csv\")\n",
    "\n",
    "    # Dataset\n",
    "    train_x, valid_x, test_x = load_data(dataset_path)\n",
    "    print(f\"Train: {len(train_x)} - Valid: {len(valid_x)} - Test: {len(test_x)}\")\n",
    "\n",
    "    # process_image_label(train_x[0])\n",
    "\n",
    "    \n",
    "    train_ds = tf_dataset(train_x, batch = hp[\"batch_size\"])\n",
    "    valid_ds = tf_dataset(valid_x, batch = hp[\"batch_size\"])\n",
    "\n",
    "    for x, y in train_ds.take(1):\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "\n",
    "    # Model\n",
    "    model = ViT(hp)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", # Because its a multi-class classification\n",
    "        optimizer = tf.keras.optimizers.Adam(hp[\"learning_rate\"], clipvalue=1.0),\n",
    "        metrics=[\"acc\"]\n",
    "    )\n",
    "    \n",
    "    callbacks = [\n",
    "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
    "        CSVLogger(csv_path),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10),\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=valid_ds,\n",
    "        epochs=hp[\"num_epochs\"],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # # Train the model with tqdm progress bar\n",
    "    # with tqdm(total=hp[\"num_epochs\"], desc=\"Training\") as pbar:\n",
    "    #     for epoch in range(hp[\"num_epochs\"]):\n",
    "    #         model.fit(\n",
    "    #             train_ds,\n",
    "    #             epochs=1,  # Train for 1 epoch at a time\n",
    "    #             validation_data=valid_ds,\n",
    "    #             verbose=0  # Set verbose to 0 to suppress TensorFlow's progress bar\n",
    "    #         )\n",
    "    #         pbar.update(1)  # Update tqdm progress bar for each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db89e63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
